{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPYChintSUKdz8egqXCcqsn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vranade-cyber/11785_Part-2/blob/main/DL2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yisdNnlwTlJ"
      },
      "source": [
        "#These libraries help to interact with the operating system and the runtime environment respectively\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "#Model/Training related libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "#Dataloader libraries\n",
        "from torch.utils.data import DataLoader, Dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXfZguN7wW15"
      },
      "source": [
        "#Intall Kaggle API and create kaggle directory\n",
        "\n",
        "!pip install --upgrade pip\n",
        "!pip install kaggle==1.5.6\n",
        "\n",
        "!mkdir .kaggle\n",
        "\n",
        "#This data is used to login  into your Kaggle account\n",
        "import json\n",
        "token = {\"username\":\"virajranade\",\"key\":\"152c772ecb15eebcb68eb988aaf4e2fd\"}\n",
        "with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSL7kwrQwbmq"
      },
      "source": [
        "import json\n",
        "token = {\"username\":\"virajranade\",\"key\":\"152c772ecb15eebcb68eb988aaf4e2fd\"}\n",
        "with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyGd6qbcwbuF"
      },
      "source": [
        "\n",
        "!chmod 600 /content/.kaggle/kaggle.json\n",
        "!cp /content/.kaggle/kaggle.json /root/.kaggle/\n",
        "import json\n",
        "token = {\"username\":\"virajranade\",\"key\":\"152c772ecb15eebcb68eb988aaf4e2fd\"}\n",
        "with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)\n",
        "!kaggle config set -n path -v /content\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ygLga9Swbxl"
      },
      "source": [
        "!kaggle competitions download -c idl-fall21-hw2p2s1-face-classification\n",
        "!kaggle competitions download -c idl-fall21-hw2p2s2-face-verification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2XM9-yewb0u"
      },
      "source": [
        "!unzip /content/competitions/idl-fall21-hw2p2s1-face-classification/idl-fall21-hw2p2s1-face-classification.zip\n",
        "#!unzip /content/test_data.zip\n",
        "!unzip /content/competitions/idl-fall21-hw2p2s2-face-verification/idl-fall21-hw2p2s2-face-verification.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0-L7Y79wb4G"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torchvision   \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h75PL8gJwb7W"
      },
      "source": [
        "imageFolder_dataset = torchvision.datasets.ImageFolder(root='/content/train_data', \n",
        "                                                      transform=(torchvision.transforms.Compose([\n",
        "                                                      torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "                                                      torchvision.transforms.RandomHorizontalFlip(0.5),torchvision.transforms.ToTensor()\n",
        "                                                      ])))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQG0GCinwb-v"
      },
      "source": [
        "imageFolder_dataloader = DataLoader(imageFolder_dataset, batch_size=32, shuffle=True, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8z9vZSYwcCU"
      },
      "source": [
        "train_dataset = torchvision.datasets.ImageFolder(root='/content/train_data',transform=(torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
        "                                                      torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "                                                      torchvision.transforms.RandomHorizontalFlip(0.25)\n",
        "                                                      ])))\n",
        "#transform=torchvision.transforms.ToTensor()\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=72, \n",
        "                                               shuffle=True, num_workers=4)\n",
        "\n",
        "map_dict=train_dataset.class_to_idx\n",
        "\n",
        "dev_dataset = torchvision.datasets.ImageFolder(root='/content/val_data',transform=(torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
        "                                                      torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "                                                      torchvision.transforms.RandomHorizontalFlip(0.25)\n",
        "                                                      ])))\n",
        "#transform=torchvision.transforms.ToTensor()\n",
        "dev_dataloader = torch.utils.data.DataLoader(dev_dataset, batch_size=72, \n",
        "                                             shuffle=False, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRyurJT9wcFX"
      },
      "source": [
        "class block(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channels, intermediate_channels, identity_downsample=None, stride=1\n",
        "    ):\n",
        "        super(block, self).__init__()\n",
        "        self.expansion = 4\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels, intermediate_channels, kernel_size=3, stride=1, padding=1, bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(intermediate_channels)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            intermediate_channels,\n",
        "            intermediate_channels,\n",
        "            kernel_size=3,\n",
        "            stride=stride,\n",
        "            padding=1,\n",
        "            bias=False\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(intermediate_channels)\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            intermediate_channels,\n",
        "            intermediate_channels * self.expansion,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "            bias=False\n",
        "        )\n",
        "        self.bn3 = nn.BatchNorm2d(intermediate_channels * self.expansion)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.identity_downsample = identity_downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x.clone()\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "\n",
        "        if self.identity_downsample is not None:\n",
        "            identity = self.identity_downsample(identity)\n",
        "\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw4XlUvaxLVd"
      },
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, image_channels, num_classes):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        #self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Essentially the entire ResNet architecture are in these 4 lines below\n",
        "        self.layer1 = self._make_layer(\n",
        "            block, layers[0], intermediate_channels=64, stride=1\n",
        "        )\n",
        "        self.layer2 = self._make_layer(\n",
        "            block, layers[1], intermediate_channels=128, stride=2\n",
        "        )\n",
        "        self.layer3 = self._make_layer(\n",
        "            block, layers[2], intermediate_channels=256, stride=2\n",
        "        )\n",
        "        self.layer4 = self._make_layer(\n",
        "            block, layers[3], intermediate_channels=512, stride=2\n",
        "        )\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * 4, num_classes)\n",
        "        self.dropout= nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        #x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        #print(x.shape)\n",
        "        embed_out=x\n",
        "        x=self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x,x\n",
        "\n",
        "    def _make_layer(self, block, num_residual_blocks, intermediate_channels, stride):\n",
        "        identity_downsample = None\n",
        "        layers = []\n",
        "\n",
        "        # Either if we half the input space for ex, 56x56 -> 28x28 (stride=2), or channels changes\n",
        "        # we need to adapt the Identity (skip connection) so it will be able to be added\n",
        "        # to the layer that's ahead\n",
        "        if stride != 1 or self.in_channels != intermediate_channels * 4:\n",
        "            identity_downsample = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    self.in_channels,\n",
        "                    intermediate_channels * 4,\n",
        "                    kernel_size=1,\n",
        "                    stride=stride,\n",
        "                    bias=False\n",
        "                ),\n",
        "                nn.BatchNorm2d(intermediate_channels * 4),\n",
        "            )\n",
        "\n",
        "        layers.append(\n",
        "            block(self.in_channels, intermediate_channels, identity_downsample, stride)\n",
        "        )\n",
        "\n",
        "        # The expansion size is always 4 for ResNet 50,101,152\n",
        "        self.in_channels = intermediate_channels * 4\n",
        "\n",
        "        # For example for first resnet layer: 256 will be mapped to 64 as intermediate layer,\n",
        "        # then finally back to 256. Hence no identity downsample is needed, since stride = 1,\n",
        "        # and also same amount of channels.\n",
        "        for i in range(num_residual_blocks - 1):\n",
        "            layers.append(block(self.in_channels, intermediate_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "573A_WdgxLcA"
      },
      "source": [
        "# This is the simplest possible residual block, with only one CNN layer.\n",
        "# Looking at the paper, you can extend this block to have more layers, bottleneck, grouped convs (from shufflenet), etc.\n",
        "# Or even look at more recent papers like resnext, regnet, resnest, senet, etc.\n",
        "class SimpleResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channel_size,out_channel_size,kernel_size=3, stride=1):\n",
        "        self.stride=stride\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channel_size=in_channel_size\n",
        "        self.out_channel_size=out_channel_size\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channel_size, out_channel_size, kernel_size=3, stride=stride,padding=1 ,bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channel_size)\n",
        "        self.conv2=nn.Conv2d(out_channel_size,out_channel_size,kernel_size=3,stride=1,padding=1,bias=False)\n",
        "        self.bn2=nn.BatchNorm2d(out_channel_size)\n",
        "\n",
        "        #self.conv3=nn.Conv2d(channel_size,channel_size*2,kernel_size=3,stride=1,padding=1,bias=False)\n",
        "        #self.bn3=nn.BatchNorm2d(channel_size*2)\n",
        "\n",
        "        if self.stride == 1:\n",
        "            self.s = nn.Identity()\n",
        "        else:\n",
        "            self.s = nn.Sequential(nn.Conv2d(self.in_channel_size, self.out_channel_size, kernel_size=3, stride=self.stride,padding=1),\n",
        "                                  nn.BatchNorm2d(self.out_channel_size))\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out=self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        '''\n",
        "        out=self.relu(out)\n",
        "\n",
        "        out=self.conv3(out)\n",
        "        out=self.bn3(out)\n",
        "        '''\n",
        "        #self.s = nn.Conv2d(self.channel_size, self.channel_size, kernel_size=3, stride=self.stride)\n",
        "        shortcut = self.s(x)\n",
        "        #print(shortcut.shape)\n",
        "        \n",
        "        self.out = self.relu(out + shortcut)\n",
        "        \n",
        "        return self.out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DomG7VcRxLgX"
      },
      "source": [
        "class ClassificationNetwork(nn.Module):\n",
        "    def __init__(self, in_features,num_classes,feat_dim = 2):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Conv2d(in_features, 64, kernel_size=3, stride=2, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            SimpleResidualBlock(64,64,2),\n",
        "            SimpleResidualBlock(64,64),\n",
        "            SimpleResidualBlock(64,64),\n",
        "\n",
        "            SimpleResidualBlock(64,128,3,2),\n",
        "            SimpleResidualBlock(128,128),\n",
        "            SimpleResidualBlock(128,128),\n",
        "            SimpleResidualBlock(128,128),\n",
        "\n",
        "\n",
        "            SimpleResidualBlock(128,256,3,2),\n",
        "            SimpleResidualBlock(256,256),\n",
        "            SimpleResidualBlock(256,256),\n",
        "            SimpleResidualBlock(256,256),\n",
        "            SimpleResidualBlock(256,256),\n",
        "            SimpleResidualBlock(256,256),\n",
        "            SimpleResidualBlock(256,256),\n",
        "\n",
        "            SimpleResidualBlock(256,512,3,2),\n",
        "            SimpleResidualBlock(512,512),\n",
        "            SimpleResidualBlock(512,512),\n",
        "\n",
        "            nn.AdaptiveAvgPool2d((1, 1)), # For each channel, collapses (averages) the entire feature map (height & width) to 1x1\n",
        "            nn.Flatten(), # the above ends up with batch_size x 64 x 1 x 1, flatten to batch_size x 64\n",
        "        )\n",
        "\n",
        "        self.linear = nn.Linear(512, feat_dim)\n",
        "        self.dropout=nn.Dropout(0.5)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.linear_output = nn.Linear(512,num_classes)\n",
        "        \n",
        "\n",
        "    def forward(self, x, return_embedding=False):\n",
        "        embedding = self.layers(x) \n",
        "        embedding_out =self.linear(embedding)\n",
        "        embedding_out=self.dropout(embedding_out)\n",
        "        embedding_out= self.relu(embedding_out)\n",
        "        output = self.linear_output(embedding)\n",
        "        if return_embedding:\n",
        "            return output,output\n",
        "        else:\n",
        "            return output  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8HNMSKWxLj7"
      },
      "source": [
        "def ResNet50(img_channel, num_classes):\n",
        "    return ResNet(block, [3, 4, 6, 3], img_channel, num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5xJVcLHxLnr"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "numEpochs = 20\n",
        "in_features = 3 # RGB channels\n",
        "image_channels=3\n",
        "\n",
        "learningRate = 0.05\n",
        "weightDecay = 5e-5\n",
        "\n",
        "feat_dim=2\n",
        "#num_classes = len(train_dataset.classes)\n",
        "num_classes = 4000\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "network = ResNet(block,[3,4,6,3],image_channels,num_classes)\n",
        "network_34= ClassificationNetwork(in_features,num_classes,feat_dim)\n",
        "#torch.cuda.empty_cache()\n",
        "network = network.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(network.parameters(), lr=learningRate, weight_decay=weightDecay, momentum=0.9)\n",
        "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,[5,10])\n",
        "scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='max',factor=0.5,patience=2)\n",
        "torch.cuda.empty_cache()\n",
        "print(network)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qyjz4SF5hNX"
      },
      "source": [
        "Save 2 Models \n",
        "1 as a Resnet 50\n",
        "Other as Resnet 34.\n",
        "Note-They require different definition.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SysWNOBtxLrI"
      },
      "source": [
        "best_model_loc = '/content/drive/MyDrive/Latest.pth'\n",
        "checkpoint = torch.load(best_model_loc)\n",
        "network.load_state_dict(checkpoint['model_state_dict'])\n",
        "print(checkpoint['epoch'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzTw1lke4y36"
      },
      "source": [
        "best_model_loc = '/content/drive/MyDrive/mymodel_34.pth'\n",
        "checkpoint = torch.load(best_model_loc)\n",
        "network_34.load_state_dict(checkpoint['model_state_dict'])\n",
        "print(checkpoint['epoch'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uc4xnD1IxLuv"
      },
      "source": [
        "print(network)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b88t_yFR_Nxp"
      },
      "source": [
        "Each epoch takes around 26 mins for Resnet 50 and 12 to 14 mins for Resnet 34\n",
        "Train both for 50 to 70 Epochs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qAP7WG_xrMD"
      },
      "source": [
        "learningRate=1e-2\n",
        "for epoch in range(50):\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    optimizer = torch.optim.SGD(network.parameters(), lr=learningRate, weight_decay=weightDecay, momentum=0.9)\n",
        "\n",
        "    # Train\n",
        "    network.train()\n",
        "    avg_loss = 0.0\n",
        "    for batch_num, (x, y) in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        network=network.to(device)\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        outputs = network(x)\n",
        "\n",
        "        loss = criterion(outputs[0], y.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_loss += loss.item()\n",
        "\n",
        "        if batch_num % 100 == 99:\n",
        "            print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch, batch_num+1, avg_loss/50))\n",
        "            \n",
        "            avg_loss = 0.0\n",
        "        filename = str(f\"mynetwork_epoch_{epoch}.pth\")\n",
        "\n",
        "    # Validate\n",
        "    network.eval()\n",
        "    num_correct = 0\n",
        "    for batch_num, (x, y) in enumerate(dev_dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        outputs,_ = network(x)\n",
        "        #print(outputs)\n",
        "        num_correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "    scheduler.step(num_correct / len(dev_dataset))\n",
        "    if (epoch+1)%2==0:\n",
        "        path=f'/content/drive/MyDrive/Colab_Notebooks/Network_models/mynetwork_epoch_{epoch}.pth'\n",
        "        torch.save({'epoch': epoch,\n",
        "        'model_state_dict': network.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict' : scheduler.state_dict()}, path)\n",
        "\n",
        "        \n",
        "    print('Epoch: {}, Validation Accuracy: {:.4f}'.format(epoch, num_correct / len(dev_dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwFdrJ9ixrQ4"
      },
      "source": [
        "def parse_data(datadir):\n",
        "    img_list = []\n",
        "    ID_list = []\n",
        "    for root, directories, filenames in os.walk(datadir):\n",
        "        print(root)  #root: median/1\n",
        "        for filename in filenames:\n",
        "            \n",
        "            if filename.endswith('.jpg'):\n",
        "                #print(filename)\n",
        "                filei = os.path.join(root, filename)\n",
        "                img_list.append(filei)\n",
        "                ID_list.append(root.split('/')[-1])\n",
        "\n",
        "    # construct a dictionary, where key and value correspond to ID and target\n",
        "    uniqueID_list = sorted(list(set(ID_list)))\n",
        "    class_n = (len(uniqueID_list))\n",
        "    target_dict = (dict(zip(uniqueID_list, range(class_n))))\n",
        "    label_list = [target_dict[ID_key] for ID_key in ID_list]\n",
        "\n",
        "    print('{}\\t\\t{}\\n{}\\t\\t{}'.format('#Images', '#Labels', len(img_list), len(set(label_list))))\n",
        "    return img_list, label_list, class_n\n",
        "    #return img_list,uniqueID_list, class_n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUi2n5RNxrUS"
      },
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, file_list, target_list):\n",
        "        self.file_list = file_list\n",
        "        self.target_list = target_list\n",
        "        self.n_class = len(list(set(target_list)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(self.file_list[index])\n",
        "        img = torchvision.transforms.ToTensor()(img)\n",
        "        label = self.target_list[index]\n",
        "        return img,label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAWT6KAAxrXd"
      },
      "source": [
        "def test_model(test_loader,network):\n",
        "    network.eval()\n",
        "    predictions=[]\n",
        "    with torch.no_grad():\n",
        "        #for batch_num, (x, y) in enumerate(test_dataloader):\n",
        "        for i in range(8000):\n",
        "          img = Image.open(f'/content/test_data/{i}.jpg')\n",
        "          #img = torchvision.transforms.ToTensor()(img) # ()\n",
        "          transform=torchvision.transforms.Compose([torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "                                                    torchvision.transforms.ToTensor()\n",
        "                                                      ])\n",
        "          img = transform(img)\n",
        "          img = torch.as_tensor(img).unsqueeze(0) # (1, 3, 64, 64)\n",
        "          img=img.to(device)\n",
        "          predicted_labels=network(img)\n",
        "          print(i)\n",
        "          #predicted_labels=pred.argmax(1)\n",
        "          #loss=criterion(pred,y)\n",
        "          #test_loss +=loss.item()\n",
        "          #correct +=(pred.argmax(1)==y).type(torch.float).sum().item()\n",
        "          predicted_labels=predicted_labels.detach()\n",
        "          predicted_labels=predicted_labels.cpu()\n",
        "          predicted_labels=predicted_labels.numpy()\n",
        "          predicted_labels = np.argmax(predicted_labels, axis=1)\n",
        "          predicted_labels= reverse_map_dict[predicted_labels[0]]\n",
        "          predicted_labels = np.reshape(predicted_labels,(-1,1))\n",
        "          predictions.append(predicted_labels)\n",
        "        #test_loss /= num_batches\n",
        "        #correct /= size\n",
        "        \n",
        "        predictions=np.vstack(predictions)\n",
        "        #print(f'\\n Test Error is ',correct*100,'Accuracy', test_loss,'Average Loss')\n",
        "        return (predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E816X4e3xrap"
      },
      "source": [
        "pred = test_model(test_dataloader, network)\n",
        "np.shape(pred)\n",
        "predictions = list(np.concatenate(pred).flat)\n",
        "ids = np.arange(len(predictions))\n",
        "len(ids)\n",
        "print(len(predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM_8dCOoxrd7"
      },
      "source": [
        "idx=np.arange((len(pred)))\n",
        "print(len(idx))\n",
        "ids=[]\n",
        "for i in idx:\n",
        "  ids.append(str(i)+'.jpg')\n",
        "preds=[int(i) for i in pred]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8RJFnKLxrgj"
      },
      "source": [
        "import pandas as pd\n",
        "my_submission = pd.DataFrame({'id': ids, 'label': preds})\n",
        "my_submission.to_csv('try9.csv', index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvqSIs_Gxrnp"
      },
      "source": [
        "key1=[]\n",
        "key2=[]\n",
        "o=0\n",
        "with open('/content/verification_pairs_test.txt') as f:\n",
        "    for line in f:\n",
        "        o=o+1\n",
        "        keys=line.split(' ')\n",
        "        #print(keys[0])\n",
        "        #print(keys[1])\n",
        "        p1=keys[0].split('/')\n",
        "        p2=keys[1].split('/')\n",
        "\n",
        "\n",
        "\n",
        "        key1.append(p1[1])\n",
        "        key2.append(p2[1])\n",
        "\n",
        "    #keys[1]\n",
        "    #print(len(key1))\n",
        "    #print(o)\n",
        "    print(key1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3No5Y_wxrra"
      },
      "source": [
        "def parse_data(datadir):\n",
        "    img_list = []\n",
        "    ID_list = []\n",
        "    for root, directories, filenames in os.walk(datadir):  #root: median/1\n",
        "        for filename in filenames:\n",
        "            if filename.endswith('.jpg'):\n",
        "                filei = os.path.join(root, filename)\n",
        "                img_list.append(filei)\n",
        "                ID_list.append(filename)\n",
        "    return img_list, ID_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9mCfcVF6wto"
      },
      "source": [
        "class Test_Dataset2(Dataset):\n",
        "    def __init__(self, file_list,target_list):\n",
        "        self.file_list = file_list\n",
        "        self.target_list=target_list\n",
        "        assert len(self.file_list)==len(self.target_list)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(self.file_list[index])\n",
        "        img = torchvision.transforms.ToTensor()(img)\n",
        "        img = torchvision.transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])(img)\n",
        "        #print(img)\n",
        "        label = self.target_list[index]\n",
        "        return img,label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSKDj7JY6wy5"
      },
      "source": [
        "img_list,ID_list=parse_data('/content/test_data')\n",
        "print(ID_list)\n",
        "print(len(img_list))\n",
        "ver_set=Test_Dataset2(img_list,ID_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJROlBKW4L4g"
      },
      "source": [
        "img1 = Image.open(img_list[0])\n",
        "img1 = torchvision.transforms.ToTensor()(img1)\n",
        "img1 = torchvision.transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])(img1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dIwEtNm4L7Y"
      },
      "source": [
        "cos=nn.CosineSimilarity(eps=1e-6)\n",
        "\n",
        "ver=[]\n",
        "ver34=[]\n",
        "id=[]\n",
        "\n",
        "\n",
        "\n",
        "for i in len(key1):\n",
        "    o1=key1[i]\n",
        "    o2=key2[i]\n",
        "\n",
        "    img1 = Image.open(img_list[o1])\n",
        "    img1 = torchvision.transforms.ToTensor()(img1)\n",
        "    img1 = torchvision.transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])(img1)\n",
        "\n",
        "    img2 = Image.open(img_list[o2])\n",
        "    img2 = torchvision.transforms.ToTensor()(img2)\n",
        "    img2 = torchvision.transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])(img2)\n",
        "    \n",
        "    _,x1=network(img1)\n",
        "    _,x2=network(img2)\n",
        "\n",
        "    _,y1=network_34(img1)\n",
        "    _,y2=network_34(img2)\n",
        "\n",
        "    C1=cos(x1,x2)\n",
        "    C2=cos(y1,y2)\n",
        "\n",
        "    ver.append(C1)\n",
        "    ver34.append(C2)\n",
        "\n",
        "    k='/verification_data/'+str(o1)+'.jpg'+'/verification_data/'+str(o2)+'.jpg'\n",
        "    id.append(k)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acYCdQxa4L-q"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "my_submission = pd.DataFrame({'Id': id, 'Category': ver})\n",
        "my_submission.to_csv('try10.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6nY0MT1-Sr7"
      },
      "source": [
        "a=0.93\n",
        "\n",
        "Ensemble=ver34*a+ ver*(1-a)\n",
        "my_submission = pd.DataFrame({'Id': id, 'Category': Ensemble})\n",
        "my_submission.to_csv('try11.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcHOVfID-2TL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}